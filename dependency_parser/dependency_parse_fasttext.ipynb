{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.tokens import DocBin, Doc\n",
    "from spacy.training import Example\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FastText model\n",
    "ft_model = gensim.models.KeyedVectors.load(\"../embeddings/sumerian_fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumerianLanguage(spacy.Language):\n",
    "    lang = \"sux\"\n",
    "\n",
    "spacy.util.registry.languages.register(\"sux\", func=SumerianLanguage)\n",
    "nlp = spacy.blank(\"sux\")\n",
    "\n",
    "def add_fasttext_vectors(nlp, ft_model):\n",
    "    nlp.vocab = Vocab()\n",
    "    for word, idx in ft_model.wv.key_to_index.items():\n",
    "        if isinstance(word, str) and word not in nlp.vocab:\n",
    "            vec = ft_model.wv.get_vector(word)\n",
    "            nlp.vocab.set_vector(word, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_fasttext_vectors(nlp, ft_model)\n",
    "config = {\n",
    "    \"min_action_freq\": 1\n",
    "}\n",
    "if \"parser\" not in nlp.pipe_names:\n",
    "    dep_parser = nlp.add_pipe(\"parser\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_data = pd.read_csv(\"../UD-ETCSUX/extra_data.csv\").dropna(subset=['sentence_id'])\n",
    "sumerian = []\n",
    "heads = []\n",
    "deps = []\n",
    "added_data = []\n",
    "sent_starts = []\n",
    "previous_page = -1\n",
    "\n",
    "for idx, row in extra_data.iterrows():\n",
    "    if previous_page != row.sentence_id:\n",
    "        if len(sumerian) > 0:\n",
    "            print(sumerian)\n",
    "            sux_text = \" \".join(sumerian)\n",
    "            sux_dict = {\"heads\": heads, \"deps\": deps, \"sent_starts\": sent_starts}\n",
    "            added_data.append((sux_text, sux_dict))\n",
    "            sumerian = []\n",
    "            heads = []\n",
    "            deps = []\n",
    "            sent_starts = []\n",
    "        sumerian.append(row.form)\n",
    "        heads.append(row[\"head\"] - 1)\n",
    "        deps.append(row.dependency)\n",
    "        sent_starts.append(1)\n",
    "        previous_page = row.sentence_id\n",
    "    else:\n",
    "        sumerian.append(row.form)\n",
    "        heads.append(row[\"head\"] - 1)\n",
    "        deps.append(row.dependency)\n",
    "        sent_starts.append(0)\n",
    "\n",
    "for idx, item in enumerate(added_data):\n",
    "    text, label = item\n",
    "    new_item = (text, text.split(\" \"), label)\n",
    "    added_data[idx] = new_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "all_data = pd.read_csv(\"../UD-ETCSUX/UD-ETCSUX.csv\").dropna(subset=['sentence_id'])\n",
    "sumerian = []\n",
    "heads = []\n",
    "deps = []\n",
    "train_data = []\n",
    "sent_starts = []\n",
    "previous_page = -1\n",
    "\n",
    "for idx, row in all_data.iterrows():\n",
    "    if previous_page != row.sentence_id:\n",
    "        if len(sumerian) > 0:\n",
    "            print(sumerian)\n",
    "            sux_text = \" \".join(sumerian)\n",
    "            sux_dict = {\"heads\": heads, \"deps\": deps, \"sent_starts\": sent_starts}\n",
    "            train_data.append((sux_text, sux_dict))\n",
    "            sumerian = []\n",
    "            heads = []\n",
    "            deps = []\n",
    "            sent_starts = []\n",
    "        print(row.form)\n",
    "        sumerian.append(row.form)\n",
    "        heads.append(row[\"head\"] - 1)\n",
    "        deps.append(row.dependency)\n",
    "        sent_starts.append(1)\n",
    "        previous_page = row.sentence_id\n",
    "    else:\n",
    "        sumerian.append(row.form)\n",
    "        heads.append(row[\"head\"] - 1)\n",
    "        deps.append(row.dependency)\n",
    "        sent_starts.append(0)\n",
    "\n",
    "for idx, item in enumerate(train_data):\n",
    "    text, label = item\n",
    "    new_item = (text, text.split(\" \"), label)\n",
    "    train_data[idx] = new_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels from your dataset\n",
    "for text, tokens, annotations in train_data:\n",
    "    for dep in annotations.get(\"deps\", []):\n",
    "        dep_parser.add_label(str(dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, tokens, annotations in added_data:\n",
    "    for dep in annotations.get(\"deps\", []):\n",
    "        dep_parser.add_label(str(dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "uas_scores = []\n",
    "las_scores = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(train_data)):\n",
    "    print(f\"Starting fold {fold + 1}\")\n",
    "    train_fold = [train_data[i] for i in train_idx]\n",
    "    test_fold = [train_data[i] for i in test_idx]\n",
    "\n",
    "    # Create training examples\n",
    "    train_examples = []\n",
    "    for text, tokens, annotations in train_fold:\n",
    "        doc = Doc(nlp.vocab, words=tokens)\n",
    "        heads = annotations['heads']\n",
    "        deps = annotations['deps']\n",
    "        train_examples.append(Example.from_dict(doc, {'heads': heads, 'deps': deps}))\n",
    "        \n",
    "    ### Uncomment this to include agumented data\n",
    "    # for text, tokens, annotations in added_data:\n",
    "    #     doc = Doc(nlp.vocab, words=tokens)\n",
    "    #     heads = annotations['heads']\n",
    "    #     deps = annotations['deps']\n",
    "    #     train_examples.append(Example.from_dict(doc, {'heads': heads, 'deps': deps}))\n",
    "\n",
    "    # Train the model\n",
    "    nlp.initialize()\n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    for i in range(20):  # Adjust the number of iterations as needed\n",
    "        losses = {}\n",
    "        batches = spacy.util.minibatch(train_examples, size=12)\n",
    "        for idx, batch in enumerate(batches):\n",
    "            try:\n",
    "                nlp.update(batch, drop=0.8, sgd=optimizer, losses=losses)\n",
    "            except Exception as e:\n",
    "                print(\"find: {}\".format(idx+1))\n",
    "                continue\n",
    "        print(f\"Losses at iteration {i}: {losses}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    uas = 0\n",
    "    las = 0\n",
    "    total = 0\n",
    "\n",
    "    for text, tokens, annotations in test_fold:\n",
    "        doc = Doc(nlp.vocab, words=tokens)\n",
    "        doc = nlp(doc)  # Apply the model to get predicted heads and deps\n",
    "        predicted_heads = [token.head.i for token in doc]\n",
    "        predicted_deps = [token.dep_ for token in doc]\n",
    "        true_heads = annotations['heads']\n",
    "        true_deps = annotations['deps']\n",
    "        for p_head, t_head, p_dep, t_dep in zip(predicted_heads, true_heads, predicted_deps, true_deps):\n",
    "            if p_head == t_head:\n",
    "                uas += 1\n",
    "            if p_head == t_head and p_dep == t_dep:\n",
    "                las += 1\n",
    "            total += 1\n",
    "\n",
    "    uas_score = uas / total\n",
    "    las_score = las / total\n",
    "\n",
    "    uas_scores.append(uas_score)\n",
    "    las_scores.append(las_score)\n",
    "\n",
    "    print(f\"Fold {fold + 1} - UAS: {uas_score:.4f}, LAS: {las_score:.4f}\")\n",
    "\n",
    "print(f\"Average UAS: {sum(uas_scores) / len(uas_scores):.4f}\")\n",
    "print(f\"Average LAS: {sum(las_scores) / len(las_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
