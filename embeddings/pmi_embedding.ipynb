{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus data\n",
    "import json\n",
    "sux_data = json.load(open(\"sux_corpus.json\"))\n",
    "\n",
    "# Convert the corpus into a suitable format\n",
    "sux_data = [[str(word) if word != 'X' else '_' for word in sentence] for sentence in sux_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sux_dict = pd.read_csv(\"../UD-ETCSUX/sux_dictionary.csv\")\n",
    "cf_dict = {}\n",
    "for idx, row in sux_dict.iterrows():\n",
    "    cf_dict[row.form] = row.cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this for lemma embedding\n",
    "new_sux_data = []\n",
    "for i in sux_data:\n",
    "    temp = []\n",
    "    for k in i:\n",
    "        if k in cf_dict:\n",
    "            temp.append(cf_dict[k])\n",
    "        else:\n",
    "            temp.append(k)\n",
    "    new_sux_data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import FastText\n",
    "import io\n",
    "\n",
    "# Sample data\n",
    "data = new_sux_data\n",
    "# Create vocabulary\n",
    "vocab = set()\n",
    "for sentence in data:\n",
    "    for word in sentence:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab = list(vocab)\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocab)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocab)}\n",
    "\n",
    "# Create sparse co-occurrence matrix\n",
    "cooc_matrix = lil_matrix((len(vocab), len(vocab)), dtype=np.float64)\n",
    "\n",
    "window_size = 2  # You can change the window size as needed\n",
    "\n",
    "for sentence in data:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    for center_i in range(len(indices)):\n",
    "        center_word = indices[center_i]\n",
    "        context_indices = list(range(max(0, center_i - window_size), min(len(indices), center_i + window_size + 1)))\n",
    "        context_indices.remove(center_i)\n",
    "        for context_i in context_indices:\n",
    "            context_word = indices[context_i]\n",
    "            cooc_matrix[center_word, context_word] += 1\n",
    "\n",
    "def calculate_pmi(cooc_matrix, positive=True):\n",
    "    total_sum = cooc_matrix.sum()\n",
    "    word_sum = np.array(cooc_matrix.sum(axis=1)).flatten()\n",
    "    pmi_matrix = lil_matrix(cooc_matrix.shape, dtype=np.float64)\n",
    "\n",
    "    rows, cols = cooc_matrix.nonzero()\n",
    "    for i, j in zip(rows, cols):\n",
    "        p_ij = cooc_matrix[i, j] / total_sum\n",
    "        p_i = word_sum[i] / total_sum\n",
    "        p_j = word_sum[j] / total_sum\n",
    "        pmi = np.log2(p_ij / (p_i * p_j))\n",
    "        if positive:\n",
    "            pmi = max(0, pmi)\n",
    "        pmi_matrix[i, j] = pmi\n",
    "\n",
    "    return pmi_matrix\n",
    "\n",
    "pmi_matrix = calculate_pmi(cooc_matrix)\n",
    "\n",
    "# Set n_components to be less than the number of unique words\n",
    "n_components = 512\n",
    "\n",
    "# Apply SVD to the PMI matrix\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "word_embeddings = svd.fit_transform(pmi_matrix)\n",
    "\n",
    "# Create a dictionary to hold the word embeddings\n",
    "word_embeddings_dict = {word: word_embeddings[word2idx[word]] for word in vocab}\n",
    "\n",
    "# Save embeddings in FastText format\n",
    "with io.open('svd_fasttext.vec', 'w', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "    f.write(f\"{len(vocab)} {n_components}\\n\")\n",
    "    for word, embedding in word_embeddings_dict.items():\n",
    "        embedding_str = ' '.join(map(str, embedding))\n",
    "        f.write(f\"{word} {embedding_str}\\n\")\n",
    "\n",
    "# Load the embeddings into a FastText model\n",
    "fasttext_model = FastText(vector_size=n_components, window=3, min_count=1)\n",
    "fasttext_model.build_vocab([list(word_embeddings_dict.keys())])\n",
    "fasttext_model.wv.vectors = np.array(list(word_embeddings_dict.values()))\n",
    "\n",
    "# Save the FastText model\n",
    "fasttext_model.save(\"svd_fasttext_lemma_model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import FastText\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the trained FastText model\n",
    "# model = FastText.load(\"svd_fasttext_lemma_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to retrieve word embeddings\n",
    "# def get_word_embedding(word):\n",
    "#     if word in model.wv:\n",
    "#         return model.wv[word]\n",
    "#     else:\n",
    "#         print(f\"Word '{word}' not in vocabulary.\")\n",
    "#         return None\n",
    "\n",
    "# # Function to compute cosine similarity between two word embeddings\n",
    "# def cosine_similarity(vec1, vec2):\n",
    "#     if vec1 is not None and vec2 is not None:\n",
    "#         return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Test words\n",
    "# word1 = 'sa₂'\n",
    "# word2 = 'in-da-sa₂'\n",
    "\n",
    "# # Retrieve embeddings\n",
    "# embedding1 = get_word_embedding(word1)\n",
    "# embedding2 = get_word_embedding(word2)\n",
    "\n",
    "# # Compute similarity\n",
    "# similarity = cosine_similarity(embedding1, embedding2)\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Embedding for '{word1}': {embedding1}\")\n",
    "# print(f\"Embedding for '{word2}': {embedding2}\")\n",
    "# print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity}\")\n",
    "\n",
    "# # Test with words not in the vocabulary\n",
    "# word3 = 'non_existent_word'\n",
    "# embedding3 = get_word_embedding(word3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
